{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Spell Correction Algorithm\n",
    "\n",
    "1. Find the misspelled word.\n",
    "2. Find strings n edit distance away from the misspelled word.\n",
    "3. Filter candidates using a word counter.\n",
    "4. Calculate word probability.\n",
    "\n",
    "This algorithm uses a dictionary of words to calculate the probability of a given word being correct. It generates a list of possible spelling corrections for a given word by generating all possible strings that are one or two edit distances away from the original word. It then filters this list using a word counter dictionary, and calculates the probability of each candidate word being correct. Finally, it returns the candidate word with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57213817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (3.7.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: setuptools in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.50.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/vuha/opt/anaconda3/lib/python3.8/site-packages (4.50.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4cf8dc",
   "metadata": {},
   "source": [
    "### Build a word counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7cc4ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2f11a8b9b813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/pipeline/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mattributeruler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttributeRuler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdep_parser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0medit_tree_lemmatizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEditTreeLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mentity_linker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mentityruler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntityRuler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/pipeline/attributeruler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBASE_EXCEPTIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mURL_MATCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlookups\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_lookups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipe_analysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manalyze_pipes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_pipe_analysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_attrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from .schemas import (\n\u001b[1;32m     45\u001b[0m     \u001b[0mConfigSchema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/pipe_analysis.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdot_to_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/tokens/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_serialize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocBin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorphanalysis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMorphAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspan_group\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpanGroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/tokens/_serialize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleFrozenList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dict_proxies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpanGroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDOCBIN_ALL_ATTRS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mALL_ATTRS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36minit spacy.vocab\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36minit spacy.tokens.doc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/schemas.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTokenPattern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0morth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStringValue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStringValue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pydantic/main.cpython-38-darwin.so\u001b[0m in \u001b[0;36mpydantic.main.ModelMetaclass.__new__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pydantic/fields.cpython-38-darwin.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pydantic/fields.cpython-38-darwin.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pydantic/fields.cpython-38-darwin.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField.prepare\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pydantic/fields.cpython-38-darwin.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField._type_analysis\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pydantic/fields.cpython-38-darwin.so\u001b[0m in \u001b[0;36mpydantic.fields.ModelField._type_analysis\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/typing.py\u001b[0m in \u001b[0;36m__subclasscheck__\u001b[0;34m(self, cls)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_special\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_GenericAlias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_special\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the spacy english model is installed\n",
    "if not spacy.util.is_package(\"en_core_web_sm\"):\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    \n",
    "# Load the english model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991ece6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACT I SCENE I.\n",
      "Enter KING HENRY, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR WALTER BLUNT, and others So shaken as we are, so wan with care, Find we a time for frighted peace to pant, And breathe short-winded accents of new broils To be commenced in strands afar remote.\n",
      "No more the thirsty entrance of this soil Shall daub her lips with her own children's blood, Nor more shall trenching war channel her fields, Nor bruise her flowerets with the armed hoofs Of hostile paces: those opposed eyes, Which, like the meteors of a troubled heaven, All of one nature, of one substance bred, Did lately meet in the intestine shock And furious close of civil butchery Shall now, in mutual well-beseeming ranks, March all one way and be no more opposed Against acquaintance, kindred and allies: The edge of war, like an ill-sheathed knife, No more shall cut his master.\n",
      "As far as to the sepulchre of Christ, Whose soldier now, under whose blessed cross We are impressed and engaged to fight, Forthwith a power of English shall we levy, Whose arms were moulded in their mothers' womb To chase these pagans in those holy fields Over whose acres walk'd those blessed feet Which fourteen hundred years ago were nail'd For our advantage on the bitter cross.\n",
      "But this our purpose now is twelve month old, And bootless 'tis to tell you we will go: Therefore we meet not now.\n",
      "Of you, my gentle cousin Westmoreland, What yesternight our council did decree In forwarding this dear expedience.\n",
      "My liege, this haste was hot in question, And many limits of the charge set down But yesternight: when all athwart there came A post from Wales loaden with heavy news, Whose worst was, that the noble Mortimer, Leading the men of Herefordshire to fight Against the irregular and wild Glendower, Was by the rude hands of that Welshman taken, A thousand of his people butchered, Upon whose dead corpse there was such misuse, Such beastly shameless transformation, By those Welshwomen done as may not be Without much shame retold or spoken of.\n",
      "It seems then that the tidings of this broil Brake off our business for the Holy Land.\n",
      "This match'd with other did, my gracious lord, For more uneven and unwelcome news Came from the north and thus it did import: On Holy-rood day, the gallant Hotspur there, Young Harry Percy and brave Archibald, That ever-valiant and approved Scot, At Holmedon met, Where they did spend a sad and bloody hour, As by discharge of their artillery, And shape of likelihood, the news was told, For he that brought them, in the very heat And pride of their contention did take horse, Uncertain of the issue any way.\n",
      "Here is a dear, a true industrious friend, Sir Walter Blunt, new lighted from his horse.\n"
     ]
    }
   ],
   "source": [
    "# The shakespeare.txt contains multiple lines of text, quoted with double quotes.\n",
    "# Let's read each line and remove the double quotes, then append the line to a list.\n",
    "read_lines = []\n",
    "with open('Data/shakespeare.txt') as f:\n",
    "    for line in f:\n",
    "        # Regular expression to remove double quotes at beginning and end of line\n",
    "        line = re.sub(r'^\"|\"\\n$', '', line)\n",
    "        read_lines.append(line)\n",
    "        \n",
    "# We will combine lines that end with a dot as a single sentence.\n",
    "sentences = []\n",
    "current_sentence = ''\n",
    "for i in range(len(read_lines)):\n",
    "    # Find the first index of a dot in the line. If it exists, append the line to the current sentence containing the dot, cut off the rest of the line, decrease the i counter by 1 and continue.\n",
    "    # If it doesn't exist, append the line to the current sentence and continue.\n",
    "    dot_index = read_lines[i].find('.')\n",
    "    \n",
    "    if dot_index != -1:\n",
    "        # Append the line to the current sentence\n",
    "        current_sentence +=  ' ' + read_lines[i][:dot_index+1]\n",
    "        # Cut off the rest of the line\n",
    "        read_lines[i] = read_lines[i][dot_index+1:]\n",
    "        # Append the current sentence to the list of sentences\n",
    "        sentences.append(current_sentence.strip())\n",
    "        # Reset the current sentence\n",
    "        current_sentence = ''\n",
    "        # Decrease the i counter by 1 so that the current line is processed again\n",
    "        i -= 1\n",
    "    else:\n",
    "        # Append the line to the current sentence\n",
    "        current_sentence += ' ' + read_lines[i]\n",
    "        \n",
    "# If the current sentence is not empty, append it to the list of sentences as well\n",
    "if current_sentence != '':\n",
    "    sentences.append(current_sentence.strip())\n",
    "    \n",
    "# Let's print the first 10 sentences\n",
    "for i in range(10):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb581f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spacy tokenizer to clean the text\n",
    "def spacy_text_clean(text):\n",
    "    \"\"\"\n",
    "    This function uses the spacy tokenizer to clean the text\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        tokens: A list of tokens that have been cleaned\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a spacy object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            tokens.append(token.lower_)\n",
    "    \n",
    "    if len(tokens) > 1:\n",
    "        return ' '.join(tokens)\n",
    "    elif len(tokens) == 1:\n",
    "        return tokens[0]\n",
    "    else:\n",
    "        ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd99e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34040 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34040/34040 [01:46<00:00, 318.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the token list\n",
    "cleaned_sentences = []\n",
    "\n",
    "for i in tqdm(range(len(sentences))):\n",
    "    cleaned_sentences.append(spacy_text_clean(sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty strings\n",
    "cleaned_sentences = [x for x in cleaned_sentences if x != '' and x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8567be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act i scene\n",
      "enter king henry lord john of lancaster the earl of westmoreland sir walter blunt and others so shaken as we are so wan with care find we a time for frighted peace to pant and breathe short winded accents of new broils to be commenced in strands afar remote\n",
      "no more the thirsty entrance of this soil shall daub her lips with her own children blood nor more shall trenching war channel her fields nor bruise her flowerets with the armed hoofs of hostile paces those opposed eyes which like the meteors of a troubled heaven all of one nature of one substance bred did lately meet in the intestine shock and furious close of civil butchery shall now in mutual well beseeming ranks march all one way and be no more opposed against acquaintance kindred and allies the edge of war like an ill sheathed knife no more shall cut his master\n",
      "as far as to the sepulchre of christ whose soldier now under whose blessed cross we are impressed and engaged to fight forthwith a power of english shall we levy whose arms were moulded in their mothers womb to chase these pagans in those holy fields over whose acres those blessed feet which fourteen hundred years ago were for our advantage on the bitter cross\n",
      "but this our purpose now is twelve month old and bootless tis to tell you we will go therefore we meet not now\n",
      "of you my gentle cousin westmoreland what yesternight our council did decree in forwarding this dear expedience\n",
      "my liege this haste was hot in question and many limits of the charge set down but yesternight when all athwart there came a post from wales loaden with heavy news whose worst was that the noble mortimer leading the men of herefordshire to fight against the irregular and wild glendower was by the rude hands of that welshman taken a thousand of his people butchered upon whose dead corpse there was such misuse such beastly shameless transformation by those welshwomen done as may not be without much shame retold or spoken of\n",
      "it seems then that the tidings of this broil brake off our business for the holy land\n",
      "this with other did my gracious lord for more uneven and unwelcome news came from the north and thus it did import on holy rood day the gallant hotspur there young harry percy and brave archibald that ever valiant and approved scot at holmedon met where they did spend a sad and bloody hour as by discharge of their artillery and shape of likelihood the news was told for he that brought them in the very heat and pride of their contention did take horse uncertain of the issue any way\n",
      "here is a dear a true industrious friend sir walter blunt new lighted from his horse\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first 10 cleaned sentences\n",
    "for i in range(10):\n",
    "    print(cleaned_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d517293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(sentences):\n",
    "    \"\"\"\n",
    "    Calculates the word counts in a list of sentences\n",
    "    \"\"\"\n",
    "    # Initialize the word counts\n",
    "    word_counts = {}\n",
    "    \n",
    "    # Split each sentence into words, and append to a list\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        words.extend(sentence.split())\n",
    "    word_counts['total'] = len(words)\n",
    "    \n",
    "    # Count the number of times each word appears\n",
    "    word_counts['counts'] = Counter(words)\n",
    "    \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb389f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words: 778667\n",
      "Top 10 most common words: [('the', 25832), ('and', 24473), ('i', 20452), ('to', 18517), ('of', 15438), ('a', 13445), ('you', 12710), ('my', 11463), ('that', 10623), ('in', 10154)]\n"
     ]
    }
   ],
   "source": [
    "# Get the word counts for the lines\n",
    "word_counts = get_word_counts(cleaned_sentences)\n",
    "\n",
    "# Print the total number of words and the top 10 most common words\n",
    "print(f'Total number of words: {word_counts[\"total\"]}')\n",
    "print(f'Top 10 most common words: {word_counts[\"counts\"].most_common(10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc8423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'aaron': 1,\n",
       " 'abaissiez': 2,\n",
       " 'abandon': 3,\n",
       " 'abandoned': 4,\n",
       " 'abase': 5,\n",
       " 'abate': 6,\n",
       " 'abated': 7,\n",
       " 'abatement': 8,\n",
       " 'abatements': 9,\n",
       " 'abates': 10,\n",
       " 'abbess': 11,\n",
       " 'abbey': 12,\n",
       " 'abbeys': 13,\n",
       " 'abbominable': 14,\n",
       " 'abbot': 15,\n",
       " 'abbots': 16,\n",
       " 'abbreviated': 17,\n",
       " 'abed': 18,\n",
       " 'abel': 19,\n",
       " 'abergavenny': 20,\n",
       " 'abet': 21,\n",
       " 'abetting': 22,\n",
       " 'abhor': 23,\n",
       " 'abhorred': 24,\n",
       " 'abhorring': 25,\n",
       " 'abhors': 26,\n",
       " 'abhorson': 27,\n",
       " 'abide': 28,\n",
       " 'abides': 29,\n",
       " 'abilities': 30,\n",
       " 'ability': 31,\n",
       " 'abject': 32,\n",
       " 'abjectly': 33,\n",
       " 'abjects': 34,\n",
       " 'abjure': 35,\n",
       " 'abjured': 36,\n",
       " 'able': 37,\n",
       " 'abler': 38,\n",
       " 'aboard': 39,\n",
       " 'abode': 40,\n",
       " 'aboded': 41,\n",
       " 'abodements': 42,\n",
       " 'aboding': 43,\n",
       " 'abominable': 44,\n",
       " 'abominably': 45,\n",
       " 'abominations': 46,\n",
       " 'abortive': 47,\n",
       " 'abortives': 48,\n",
       " 'abound': 49,\n",
       " 'abounding': 50,\n",
       " 'about': 51,\n",
       " 'above': 52,\n",
       " 'abraham': 53,\n",
       " 'abram': 54,\n",
       " 'abreast': 55,\n",
       " 'abridge': 56,\n",
       " 'abridged': 57,\n",
       " 'abridgement': 58,\n",
       " 'abridgment': 59,\n",
       " 'abroach': 60,\n",
       " 'abroad': 61,\n",
       " 'abrogate': 62,\n",
       " 'abrook': 63,\n",
       " 'abrupt': 64,\n",
       " 'abruption': 65,\n",
       " 'abruptly': 66,\n",
       " 'absence': 67,\n",
       " 'absent': 68,\n",
       " 'absey': 69,\n",
       " 'absolute': 70,\n",
       " 'absolutely': 71,\n",
       " 'absolved': 72,\n",
       " 'absolver': 73,\n",
       " 'abstains': 74,\n",
       " 'abstemious': 75,\n",
       " 'abstinence': 76,\n",
       " 'abstract': 77,\n",
       " 'absurd': 78,\n",
       " 'absyrtus': 79,\n",
       " 'abundance': 80,\n",
       " 'abundant': 81,\n",
       " 'abundantly': 82,\n",
       " 'abuse': 83,\n",
       " 'abused': 84,\n",
       " 'abuser': 85,\n",
       " 'abuses': 86,\n",
       " 'abusing': 87,\n",
       " 'abuts': 88,\n",
       " 'abutting': 89,\n",
       " 'aby': 90,\n",
       " 'abysm': 91,\n",
       " 'academe': 92,\n",
       " 'academes': 93,\n",
       " 'accent': 94,\n",
       " 'accents': 95,\n",
       " 'accept': 96,\n",
       " 'acceptance': 97,\n",
       " 'accepted': 98,\n",
       " 'accepts': 99,\n",
       " 'access': 100,\n",
       " 'accessary': 101,\n",
       " 'accessible': 102,\n",
       " 'accidence': 103,\n",
       " 'accident': 104,\n",
       " 'accidental': 105,\n",
       " 'accidentally': 106,\n",
       " 'accidents': 107,\n",
       " 'acclamations': 108,\n",
       " 'accommodate': 109,\n",
       " 'accommodated': 110,\n",
       " 'accommodation': 111,\n",
       " 'accommodations': 112,\n",
       " 'accompanied': 113,\n",
       " 'accompany': 114,\n",
       " 'accompanying': 115,\n",
       " 'accomplices': 116,\n",
       " 'accomplish': 117,\n",
       " 'accomplished': 118,\n",
       " 'accomplishing': 119,\n",
       " 'accomplishment': 120,\n",
       " 'accompt': 121,\n",
       " 'accord': 122,\n",
       " 'accordant': 123,\n",
       " 'accordeth': 124,\n",
       " 'according': 125,\n",
       " 'accordingly': 126,\n",
       " 'accords': 127,\n",
       " 'accost': 128,\n",
       " 'accosted': 129,\n",
       " 'accosting': 130,\n",
       " 'account': 131,\n",
       " 'accountant': 132,\n",
       " 'accounted': 133,\n",
       " 'accounts': 134,\n",
       " 'accoutred': 135,\n",
       " 'accoutrement': 136,\n",
       " 'accoutrements': 137,\n",
       " 'accrue': 138,\n",
       " 'accumulate': 139,\n",
       " 'accumulated': 140,\n",
       " 'accumulation': 141,\n",
       " 'accursed': 142,\n",
       " 'accurst': 143,\n",
       " 'accusation': 144,\n",
       " 'accusations': 145,\n",
       " 'accusative': 146,\n",
       " 'accusativo': 147,\n",
       " 'accuse': 148,\n",
       " 'accused': 149,\n",
       " 'accuser': 150,\n",
       " 'accusers': 151,\n",
       " 'accuses': 152,\n",
       " 'accuseth': 153,\n",
       " 'accusing': 154,\n",
       " 'accustomed': 155,\n",
       " 'ace': 156,\n",
       " 'ache': 157,\n",
       " 'acheron': 158,\n",
       " 'aches': 159,\n",
       " 'achieve': 160,\n",
       " 'achieved': 161,\n",
       " 'achievement': 162,\n",
       " 'achievements': 163,\n",
       " 'achiever': 164,\n",
       " 'achieves': 165,\n",
       " 'achieving': 166,\n",
       " 'achilles': 167,\n",
       " 'aching': 168,\n",
       " 'acknowledge': 169,\n",
       " 'acknowledged': 170,\n",
       " 'acknowledgement': 171,\n",
       " 'acknown': 172,\n",
       " 'acordo': 173,\n",
       " 'acorn': 174,\n",
       " 'acquaint': 175,\n",
       " 'acquaintance': 176,\n",
       " 'acquainted': 177,\n",
       " 'acquaints': 178,\n",
       " 'acquire': 179,\n",
       " 'acquired': 180,\n",
       " 'acquisition': 181,\n",
       " 'acquit': 182,\n",
       " 'acquittance': 183,\n",
       " 'acquittances': 184,\n",
       " 'acquitted': 185,\n",
       " 'acre': 186,\n",
       " 'acres': 187,\n",
       " 'across': 188,\n",
       " 'act': 189,\n",
       " 'actaeon': 190,\n",
       " 'acted': 191,\n",
       " 'acting': 192,\n",
       " 'action': 193,\n",
       " 'actions': 194,\n",
       " 'actium': 195,\n",
       " 'active': 196,\n",
       " 'actively': 197,\n",
       " 'activity': 198,\n",
       " 'actor': 199,\n",
       " 'actors': 200,\n",
       " 'acts': 201,\n",
       " 'actual': 202,\n",
       " 'acute': 203,\n",
       " 'acutely': 204,\n",
       " 'ad': 205,\n",
       " 'adage': 206,\n",
       " 'adallas': 207,\n",
       " 'adam': 208,\n",
       " 'adamant': 209,\n",
       " 'add': 210,\n",
       " 'added': 211,\n",
       " 'adder': 212,\n",
       " 'adders': 213,\n",
       " 'addicted': 214,\n",
       " 'addiction': 215,\n",
       " 'adding': 216,\n",
       " 'addition': 217,\n",
       " 'additions': 218,\n",
       " 'addle': 219,\n",
       " 'address': 220,\n",
       " 'addrest': 221,\n",
       " 'adds': 222,\n",
       " 'adhere': 223,\n",
       " 'adheres': 224,\n",
       " 'adieu': 225,\n",
       " 'adieus': 226,\n",
       " 'adjacent': 227,\n",
       " 'adjoining': 228,\n",
       " 'adjourn': 229,\n",
       " 'adjudged': 230,\n",
       " 'adjunct': 231,\n",
       " 'administer': 232,\n",
       " 'admirable': 233,\n",
       " 'admiral': 234,\n",
       " 'admiration': 235,\n",
       " 'admire': 236,\n",
       " 'admired': 237,\n",
       " 'admirer': 238,\n",
       " 'admiring': 239,\n",
       " 'admiringly': 240,\n",
       " 'admission': 241,\n",
       " 'admit': 242,\n",
       " 'admits': 243,\n",
       " 'admittance': 244,\n",
       " 'admitted': 245,\n",
       " 'admitting': 246,\n",
       " 'admonish': 247,\n",
       " 'admonishing': 248,\n",
       " 'admonishment': 249,\n",
       " 'admonishments': 250,\n",
       " 'admonition': 251,\n",
       " 'ado': 252,\n",
       " 'adonis': 253,\n",
       " 'adopt': 254,\n",
       " 'adopted': 255,\n",
       " 'adoptedly': 256,\n",
       " 'adoption': 257,\n",
       " 'adoptious': 258,\n",
       " 'adopts': 259,\n",
       " 'adoration': 260,\n",
       " 'adorations': 261,\n",
       " 'adore': 262,\n",
       " 'adored': 263,\n",
       " 'adorer': 264,\n",
       " 'adores': 265,\n",
       " 'adorest': 266,\n",
       " 'adoreth': 267,\n",
       " 'adoring': 268,\n",
       " 'adorn': 269,\n",
       " 'adorned': 270,\n",
       " 'adornings': 271,\n",
       " 'adornment': 272,\n",
       " 'adorns': 273,\n",
       " 'adown': 274,\n",
       " 'adramadio': 275,\n",
       " 'adrian': 276,\n",
       " 'adriana': 277,\n",
       " 'adriano': 278,\n",
       " 'adriatic': 279,\n",
       " 'adsum': 280,\n",
       " 'adulation': 281,\n",
       " 'adulterate': 282,\n",
       " 'adulterates': 283,\n",
       " 'adulterers': 284,\n",
       " 'adulteress': 285,\n",
       " 'adulteries': 286,\n",
       " 'adulterous': 287,\n",
       " 'adultery': 288,\n",
       " 'adultress': 289,\n",
       " 'advance': 290,\n",
       " 'advanced': 291,\n",
       " 'advancement': 292,\n",
       " 'advances': 293,\n",
       " 'advancing': 294,\n",
       " 'advantage': 295,\n",
       " 'advantageable': 296,\n",
       " 'advantaged': 297,\n",
       " 'advantageous': 298,\n",
       " 'advantages': 299,\n",
       " 'advantaging': 300,\n",
       " 'adventure': 301,\n",
       " 'adventured': 302,\n",
       " 'adventures': 303,\n",
       " 'adventuring': 304,\n",
       " 'adventurous': 305,\n",
       " 'adventurously': 306,\n",
       " 'adversaries': 307,\n",
       " 'adversary': 308,\n",
       " 'adverse': 309,\n",
       " 'adversely': 310,\n",
       " 'adversities': 311,\n",
       " 'adversity': 312,\n",
       " 'advertise': 313,\n",
       " 'advertised': 314,\n",
       " 'advertisement': 315,\n",
       " 'advertising': 316,\n",
       " 'advice': 317,\n",
       " 'advise': 318,\n",
       " 'advised': 319,\n",
       " 'advisedly': 320,\n",
       " 'advises': 321,\n",
       " 'advisings': 322,\n",
       " 'advocate': 323,\n",
       " 'advocation': 324,\n",
       " 'aeacida': 325,\n",
       " 'aeacides': 326,\n",
       " 'aedile': 327,\n",
       " 'aediles': 328,\n",
       " 'aegeon': 329,\n",
       " 'aegle': 330,\n",
       " 'aemilia': 331,\n",
       " 'aemilius': 332,\n",
       " 'aeneas': 333,\n",
       " 'aeolus': 334,\n",
       " 'aer': 335,\n",
       " 'aerial': 336,\n",
       " 'aery': 337,\n",
       " 'aesculapius': 338,\n",
       " 'aeson': 339,\n",
       " 'aesop': 340,\n",
       " 'aetna': 341,\n",
       " 'afar': 342,\n",
       " 'afeard': 343,\n",
       " 'afeared': 344,\n",
       " 'affability': 345,\n",
       " 'affable': 346,\n",
       " 'affair': 347,\n",
       " 'affaire': 348,\n",
       " 'affairs': 349,\n",
       " 'affect': 350,\n",
       " 'affectation': 351,\n",
       " 'affectations': 352,\n",
       " 'affected': 353,\n",
       " 'affecteth': 354,\n",
       " 'affecting': 355,\n",
       " 'affection': 356,\n",
       " 'affectionate': 357,\n",
       " 'affectionately': 358,\n",
       " 'affectioned': 359,\n",
       " 'affections': 360,\n",
       " 'affects': 361,\n",
       " 'affiance': 362,\n",
       " 'affianced': 363,\n",
       " 'affied': 364,\n",
       " 'affined': 365,\n",
       " 'affinity': 366,\n",
       " 'affirm': 367,\n",
       " 'affirmation': 368,\n",
       " 'affirmatives': 369,\n",
       " 'afflict': 370,\n",
       " 'afflicted': 371,\n",
       " 'affliction': 372,\n",
       " 'afflictions': 373,\n",
       " 'afflicts': 374,\n",
       " 'afford': 375,\n",
       " 'affordeth': 376,\n",
       " 'affords': 377,\n",
       " 'affray': 378,\n",
       " 'affright': 379,\n",
       " 'affrighted': 380,\n",
       " 'affrights': 381,\n",
       " 'affront': 382,\n",
       " 'affronted': 383,\n",
       " 'affy': 384,\n",
       " 'afield': 385,\n",
       " 'afire': 386,\n",
       " 'afloat': 387,\n",
       " 'afoot': 388,\n",
       " 'afore': 389,\n",
       " 'aforehand': 390,\n",
       " 'aforesaid': 391,\n",
       " 'afraid': 392,\n",
       " 'afresh': 393,\n",
       " 'afric': 394,\n",
       " 'african': 395,\n",
       " 'after': 396,\n",
       " 'afternoon': 397,\n",
       " 'afterward': 398,\n",
       " 'afterwards': 399,\n",
       " 'again': 400,\n",
       " 'against': 401,\n",
       " 'agamemnon': 402,\n",
       " 'agate': 403,\n",
       " 'agazed': 404,\n",
       " 'age': 405,\n",
       " 'aged': 406,\n",
       " 'agenor': 407,\n",
       " 'agent': 408,\n",
       " 'agents': 409,\n",
       " 'ages': 410,\n",
       " 'aggravate': 411,\n",
       " 'aggrieved': 412,\n",
       " 'agile': 413,\n",
       " 'agincourt': 414,\n",
       " 'agitation': 415,\n",
       " 'aglet': 416,\n",
       " 'agnise': 417,\n",
       " 'ago': 418,\n",
       " 'agone': 419,\n",
       " 'agony': 420,\n",
       " 'agood': 421,\n",
       " 'agree': 422,\n",
       " 'agreed': 423,\n",
       " 'agreeing': 424,\n",
       " 'agreement': 425,\n",
       " 'agrees': 426,\n",
       " 'agrippa': 427,\n",
       " 'aground': 428,\n",
       " 'ague': 429,\n",
       " 'aguecheek': 430,\n",
       " 'agued': 431,\n",
       " 'agueface': 432,\n",
       " 'agues': 433,\n",
       " 'ah': 434,\n",
       " 'ai': 435,\n",
       " 'aid': 436,\n",
       " 'aidance': 437,\n",
       " 'aidant': 438,\n",
       " 'aided': 439,\n",
       " 'aiding': 440,\n",
       " 'aidless': 441,\n",
       " 'ail': 442,\n",
       " 'ailest': 443,\n",
       " 'aim': 444,\n",
       " 'aimed': 445,\n",
       " 'aimest': 446,\n",
       " 'aiming': 447,\n",
       " 'aims': 448,\n",
       " 'ainsi': 449,\n",
       " 'aio': 450,\n",
       " 'air': 451,\n",
       " 'aired': 452,\n",
       " 'airless': 453,\n",
       " 'airs': 454,\n",
       " 'airy': 455,\n",
       " 'ajax': 456,\n",
       " 'alabaster': 457,\n",
       " 'alack': 458,\n",
       " 'alacrity': 459,\n",
       " 'alarbus': 460,\n",
       " 'alarm': 461,\n",
       " 'alarms': 462,\n",
       " 'alarum': 463,\n",
       " 'alarums': 464,\n",
       " 'alas': 465,\n",
       " 'alban': 466,\n",
       " 'albany': 467,\n",
       " 'albeit': 468,\n",
       " 'albion': 469,\n",
       " 'alchemist': 470,\n",
       " 'alchemy': 471,\n",
       " 'alcibiades': 472,\n",
       " 'alcides': 473,\n",
       " 'alder': 474,\n",
       " 'alderman': 475,\n",
       " 'aldermen': 476,\n",
       " 'ale': 477,\n",
       " 'alehouse': 478,\n",
       " 'alencon': 479,\n",
       " 'aleppo': 480,\n",
       " 'ales': 481,\n",
       " 'alexander': 482,\n",
       " 'alexanders': 483,\n",
       " 'alexandria': 484,\n",
       " 'alexandrian': 485,\n",
       " 'alexas': 486,\n",
       " 'alias': 487,\n",
       " 'alice': 488,\n",
       " 'alien': 489,\n",
       " 'aliena': 490,\n",
       " 'alight': 491,\n",
       " 'alighted': 492,\n",
       " 'alike': 493,\n",
       " 'alisander': 494,\n",
       " 'alit': 495,\n",
       " 'alive': 496,\n",
       " 'all': 497,\n",
       " 'alla': 498,\n",
       " 'allay': 499,\n",
       " 'allaying': 500,\n",
       " 'allayment': 501,\n",
       " 'allayments': 502,\n",
       " 'allays': 503,\n",
       " 'allegation': 504,\n",
       " 'allegations': 505,\n",
       " 'allege': 506,\n",
       " 'alleged': 507,\n",
       " 'allegiance': 508,\n",
       " 'allegiant': 509,\n",
       " 'alley': 510,\n",
       " 'alleys': 511,\n",
       " 'alliance': 512,\n",
       " 'allicholy': 513,\n",
       " 'allied': 514,\n",
       " 'allies': 515,\n",
       " 'alligant': 516,\n",
       " 'alligator': 517,\n",
       " 'allons': 518,\n",
       " 'allot': 519,\n",
       " 'allotted': 520,\n",
       " 'allottery': 521,\n",
       " 'allow': 522,\n",
       " 'allowance': 523,\n",
       " 'allowed': 524,\n",
       " 'allowing': 525,\n",
       " 'allows': 526,\n",
       " 'allure': 527,\n",
       " 'allured': 528,\n",
       " 'allurement': 529,\n",
       " 'alluring': 530,\n",
       " 'allusion': 531,\n",
       " 'ally': 532,\n",
       " 'allycholly': 533,\n",
       " 'almain': 534,\n",
       " 'almanac': 535,\n",
       " 'almanacs': 536,\n",
       " 'almighty': 537,\n",
       " 'almond': 538,\n",
       " 'almost': 539,\n",
       " 'alms': 540,\n",
       " 'almshouses': 541,\n",
       " 'almsman': 542,\n",
       " 'aloft': 543,\n",
       " 'alone': 544,\n",
       " 'along': 545,\n",
       " 'alonso': 546,\n",
       " 'aloof': 547,\n",
       " 'aloud': 548,\n",
       " 'alphabet': 549,\n",
       " 'alphabetical': 550,\n",
       " 'alphonso': 551,\n",
       " 'alps': 552,\n",
       " 'already': 553,\n",
       " 'also': 554,\n",
       " 'altar': 555,\n",
       " 'altars': 556,\n",
       " 'alter': 557,\n",
       " 'alteration': 558,\n",
       " 'altered': 559,\n",
       " 'altering': 560,\n",
       " 'alters': 561,\n",
       " 'althaea': 562,\n",
       " 'although': 563,\n",
       " 'altitude': 564,\n",
       " 'altogether': 565,\n",
       " 'alton': 566,\n",
       " 'alway': 567,\n",
       " 'always': 568,\n",
       " 'am': 569,\n",
       " 'amain': 570,\n",
       " 'amamon': 571,\n",
       " 'amaze': 572,\n",
       " 'amazed': 573,\n",
       " 'amazedly': 574,\n",
       " 'amazedness': 575,\n",
       " 'amazement': 576,\n",
       " 'amazes': 577,\n",
       " 'amazing': 578,\n",
       " 'amazon': 579,\n",
       " 'amazonian': 580,\n",
       " 'amazons': 581,\n",
       " 'ambassador': 582,\n",
       " 'ambassadors': 583,\n",
       " 'amber': 584,\n",
       " 'ambiguities': 585,\n",
       " 'ambiguous': 586,\n",
       " 'ambition': 587,\n",
       " 'ambitions': 588,\n",
       " 'ambitious': 589,\n",
       " 'ambitiously': 590,\n",
       " 'amble': 591,\n",
       " 'ambled': 592,\n",
       " 'ambles': 593,\n",
       " 'ambling': 594,\n",
       " 'ambuscadoes': 595,\n",
       " 'ambush': 596,\n",
       " 'amen': 597,\n",
       " 'amend': 598,\n",
       " 'amended': 599,\n",
       " 'amendment': 600,\n",
       " 'amends': 601,\n",
       " 'amerce': 602,\n",
       " 'america': 603,\n",
       " 'ames': 604,\n",
       " 'amiable': 605,\n",
       " 'amid': 606,\n",
       " 'amidst': 607,\n",
       " 'amiens': 608,\n",
       " 'amiss': 609,\n",
       " 'amities': 610,\n",
       " 'amity': 611,\n",
       " 'among': 612,\n",
       " 'amongst': 613,\n",
       " 'amorous': 614,\n",
       " 'amort': 615,\n",
       " 'amount': 616,\n",
       " 'amounts': 617,\n",
       " 'amphimachus': 618,\n",
       " 'ample': 619,\n",
       " 'ampler': 620,\n",
       " 'amplest': 621,\n",
       " 'amplified': 622,\n",
       " 'amplify': 623,\n",
       " 'amply': 624,\n",
       " 'ampthill': 625,\n",
       " 'amyntas': 626,\n",
       " 'an': 627,\n",
       " 'anatomize': 628,\n",
       " 'anatomized': 629,\n",
       " 'anatomy': 630,\n",
       " 'ancestor': 631,\n",
       " 'ancestors': 632,\n",
       " 'ancestry': 633,\n",
       " 'anchises': 634,\n",
       " 'anchor': 635,\n",
       " 'anchorage': 636,\n",
       " 'anchoring': 637,\n",
       " 'anchors': 638,\n",
       " 'anchovies': 639,\n",
       " 'ancient': 640,\n",
       " 'ancientry': 641,\n",
       " 'ancients': 642,\n",
       " 'ancle': 643,\n",
       " 'ancus': 644,\n",
       " 'and': 645,\n",
       " 'andren': 646,\n",
       " 'andrew': 647,\n",
       " 'andromache': 648,\n",
       " 'andronici': 649,\n",
       " 'andronicus': 650,\n",
       " 'anew': 651,\n",
       " 'angel': 652,\n",
       " 'angelica': 653,\n",
       " 'angelical': 654,\n",
       " 'angelo': 655,\n",
       " 'angels': 656,\n",
       " 'anger': 657,\n",
       " 'angering': 658,\n",
       " 'angerly': 659,\n",
       " 'angers': 660,\n",
       " 'anges': 661,\n",
       " 'angiers': 662,\n",
       " 'angle': 663,\n",
       " 'angled': 664,\n",
       " 'angler': 665,\n",
       " 'angleterre': 666,\n",
       " 'angliae': 667,\n",
       " 'angling': 668,\n",
       " 'anglish': 669,\n",
       " 'anglois': 670,\n",
       " 'angry': 671,\n",
       " 'anguish': 672,\n",
       " 'angus': 673,\n",
       " 'animal': 674,\n",
       " 'animals': 675,\n",
       " 'animis': 676,\n",
       " 'anjou': 677,\n",
       " 'anna': 678,\n",
       " 'annals': 679,\n",
       " 'anne': 680,\n",
       " 'annexment': 681,\n",
       " 'annothanize': 682,\n",
       " 'annoy': 683,\n",
       " 'annoyance': 684,\n",
       " 'annoying': 685,\n",
       " 'annual': 686,\n",
       " 'anoint': 687,\n",
       " 'anointed': 688,\n",
       " 'anon': 689,\n",
       " 'another': 690,\n",
       " 'anselme': 691,\n",
       " 'answer': 692,\n",
       " 'answerable': 693,\n",
       " 'answered': 694,\n",
       " 'answerer': 695,\n",
       " 'answerest': 696,\n",
       " 'answering': 697,\n",
       " 'answers': 698,\n",
       " 'ant': 699,\n",
       " 'ante': 700,\n",
       " 'antenor': 701,\n",
       " 'antenorides': 702,\n",
       " 'anthem': 703,\n",
       " 'anthony': 704,\n",
       " 'anthropophagi': 705,\n",
       " 'anthropophaginian': 706,\n",
       " 'antiates': 707,\n",
       " 'antic': 708,\n",
       " 'anticipates': 709,\n",
       " 'anticipatest': 710,\n",
       " 'anticipating': 711,\n",
       " 'anticipation': 712,\n",
       " 'anticly': 713,\n",
       " 'antics': 714,\n",
       " 'antidote': 715,\n",
       " 'antidotes': 716,\n",
       " 'antigonus': 717,\n",
       " 'antioch': 718,\n",
       " 'antiochus': 719,\n",
       " 'antiopa': 720,\n",
       " 'antipathy': 721,\n",
       " 'antipholus': 722,\n",
       " 'antipholuses': 723,\n",
       " 'antipodes': 724,\n",
       " 'antiquary': 725,\n",
       " 'antique': 726,\n",
       " 'antiquity': 727,\n",
       " 'antiquius': 728,\n",
       " 'antium': 729,\n",
       " 'antoniad': 730,\n",
       " 'antonio': 731,\n",
       " 'antonius': 732,\n",
       " 'antony': 733,\n",
       " 'antres': 734,\n",
       " 'anvil': 735,\n",
       " 'any': 736,\n",
       " 'anything': 737,\n",
       " 'ap': 738,\n",
       " 'apace': 739,\n",
       " 'apart': 740,\n",
       " 'apartments': 741,\n",
       " 'ape': 742,\n",
       " 'apemantus': 743,\n",
       " 'apennines': 744,\n",
       " 'apes': 745,\n",
       " 'apex': 746,\n",
       " 'apiece': 747,\n",
       " 'apish': 748,\n",
       " 'apollinem': 749,\n",
       " 'apollo': 750,\n",
       " 'apollodorus': 751,\n",
       " 'apollos': 752,\n",
       " 'apology': 753,\n",
       " 'apostle': 754,\n",
       " 'apostles': 755,\n",
       " 'apostraphas': 756,\n",
       " 'apothecary': 757,\n",
       " 'appal': 758,\n",
       " 'appalled': 759,\n",
       " 'appals': 760,\n",
       " 'apparel': 761,\n",
       " 'apparelled': 762,\n",
       " 'apparent': 763,\n",
       " 'apparently': 764,\n",
       " 'apparition': 765,\n",
       " 'apparitions': 766,\n",
       " 'appeach': 767,\n",
       " 'appeal': 768,\n",
       " 'appeals': 769,\n",
       " 'appear': 770,\n",
       " 'appearance': 771,\n",
       " 'appeared': 772,\n",
       " 'appearer': 773,\n",
       " 'appeareth': 774,\n",
       " 'appearing': 775,\n",
       " 'appears': 776,\n",
       " 'appease': 777,\n",
       " 'appeased': 778,\n",
       " 'appele': 779,\n",
       " 'appelee': 780,\n",
       " 'appeles': 781,\n",
       " 'appelez': 782,\n",
       " 'appellant': 783,\n",
       " 'appellants': 784,\n",
       " 'appelons': 785,\n",
       " 'appendix': 786,\n",
       " 'apperil': 787,\n",
       " 'appertain': 788,\n",
       " 'appertaining': 789,\n",
       " 'appertainments': 790,\n",
       " 'appertains': 791,\n",
       " 'appertinent': 792,\n",
       " 'appertinents': 793,\n",
       " 'appetite': 794,\n",
       " 'appetites': 795,\n",
       " 'applaud': 796,\n",
       " 'applauded': 797,\n",
       " 'applauding': 798,\n",
       " 'applause': 799,\n",
       " 'applauses': 800,\n",
       " 'apple': 801,\n",
       " 'apples': 802,\n",
       " 'appliance': 803,\n",
       " 'appliances': 804,\n",
       " 'applications': 805,\n",
       " 'applied': 806,\n",
       " 'applies': 807,\n",
       " 'apply': 808,\n",
       " 'applying': 809,\n",
       " 'appoint': 810,\n",
       " 'appointed': 811,\n",
       " 'appointment': 812,\n",
       " 'appointments': 813,\n",
       " 'appoints': 814,\n",
       " 'apprehend': 815,\n",
       " 'apprehended': 816,\n",
       " 'apprehendest': 817,\n",
       " 'apprehends': 818,\n",
       " 'apprehension': 819,\n",
       " 'apprehensions': 820,\n",
       " 'apprehensive': 821,\n",
       " 'apprenticehood': 822,\n",
       " 'appris': 823,\n",
       " 'approach': 824,\n",
       " 'approached': 825,\n",
       " 'approachers': 826,\n",
       " 'approaches': 827,\n",
       " 'approacheth': 828,\n",
       " 'approaching': 829,\n",
       " 'approbation': 830,\n",
       " 'approof': 831,\n",
       " 'appropriation': 832,\n",
       " 'approve': 833,\n",
       " 'approved': 834,\n",
       " 'approvers': 835,\n",
       " 'approves': 836,\n",
       " 'appurtenance': 837,\n",
       " 'appurtenances': 838,\n",
       " 'apricocks': 839,\n",
       " 'april': 840,\n",
       " 'apron': 841,\n",
       " 'aprons': 842,\n",
       " 'apt': 843,\n",
       " 'apter': 844,\n",
       " 'aptly': 845,\n",
       " 'aptness': 846,\n",
       " 'aqua': 847,\n",
       " 'aquilon': 848,\n",
       " 'aquitaine': 849,\n",
       " 'arabia': 850,\n",
       " 'arabian': 851,\n",
       " 'araise': 852,\n",
       " 'arbitrate': 853,\n",
       " 'arbitrating': 854,\n",
       " 'arbitrator': 855,\n",
       " 'arbitrement': 856,\n",
       " 'arbour': 857,\n",
       " 'arbours': 858,\n",
       " 'arc': 859,\n",
       " 'arch': 860,\n",
       " 'archbishop': 861,\n",
       " 'archbishopric': 862,\n",
       " 'archdeacon': 863,\n",
       " 'arched': 864,\n",
       " 'archelaus': 865,\n",
       " 'archer': 866,\n",
       " 'archers': 867,\n",
       " 'archery': 868,\n",
       " 'archibald': 869,\n",
       " 'archidamus': 870,\n",
       " 'architect': 871,\n",
       " 'arcu': 872,\n",
       " 'arde': 873,\n",
       " 'arden': 874,\n",
       " 'ardent': 875,\n",
       " 'ardour': 876,\n",
       " 'are': 877,\n",
       " 'argal': 878,\n",
       " 'argentine': 879,\n",
       " 'argier': 880,\n",
       " 'argo': 881,\n",
       " 'argosies': 882,\n",
       " 'argosy': 883,\n",
       " 'argue': 884,\n",
       " 'argued': 885,\n",
       " 'argues': 886,\n",
       " 'arguing': 887,\n",
       " 'argument': 888,\n",
       " 'arguments': 889,\n",
       " 'argus': 890,\n",
       " 'ariachne': 891,\n",
       " 'ariadne': 892,\n",
       " 'ariel': 893,\n",
       " 'aries': 894,\n",
       " 'aright': 895,\n",
       " 'arion': 896,\n",
       " 'arise': 897,\n",
       " 'arises': 898,\n",
       " 'ariseth': 899,\n",
       " 'aristotle': 900,\n",
       " 'arithmetic': 901,\n",
       " 'arithmetician': 902,\n",
       " 'ark': 903,\n",
       " 'arm': 904,\n",
       " 'arma': 905,\n",
       " 'armado': 906,\n",
       " 'armadoes': 907,\n",
       " 'armagnac': 908,\n",
       " 'arme': 909,\n",
       " 'armed': 910,\n",
       " 'armenia': 911,\n",
       " 'armies': 912,\n",
       " 'armigero': 913,\n",
       " 'arming': 914,\n",
       " 'armipotent': 915,\n",
       " 'armory': 916,\n",
       " 'armour': 917,\n",
       " 'armourer': 918,\n",
       " 'armourers': 919,\n",
       " 'armours': 920,\n",
       " 'armoury': 921,\n",
       " 'arms': 922,\n",
       " 'army': 923,\n",
       " 'aroint': 924,\n",
       " 'arose': 925,\n",
       " 'arouse': 926,\n",
       " 'aroused': 927,\n",
       " 'arragon': 928,\n",
       " 'arraign': 929,\n",
       " 'arraigned': 930,\n",
       " 'arraigning': 931,\n",
       " 'arraignment': 932,\n",
       " 'arrant': 933,\n",
       " 'arras': 934,\n",
       " 'array': 935,\n",
       " 'arrearages': 936,\n",
       " 'arrest': 937,\n",
       " 'arrested': 938,\n",
       " 'arrests': 939,\n",
       " 'arrival': 940,\n",
       " 'arrivance': 941,\n",
       " 'arrive': 942,\n",
       " 'arrived': 943,\n",
       " 'arrives': 944,\n",
       " 'arriving': 945,\n",
       " 'arrogance': 946,\n",
       " 'arrogancy': 947,\n",
       " 'arrogant': 948,\n",
       " 'arrow': 949,\n",
       " 'arrows': 950,\n",
       " 'art': 951,\n",
       " 'artemidorus': 952,\n",
       " 'arteries': 953,\n",
       " 'artery': 954,\n",
       " 'arthur': 955,\n",
       " 'article': 956,\n",
       " 'articles': 957,\n",
       " 'articulate': 958,\n",
       " 'artificer': 959,\n",
       " 'artificial': 960,\n",
       " 'artillery': 961,\n",
       " 'artist': 962,\n",
       " 'artless': 963,\n",
       " 'artois': 964,\n",
       " 'arts': 965,\n",
       " 'artus': 966,\n",
       " 'arviragus': 967,\n",
       " 'as': 968,\n",
       " 'ascanius': 969,\n",
       " 'ascend': 970,\n",
       " 'ascended': 971,\n",
       " 'ascendeth': 972,\n",
       " 'ascends': 973,\n",
       " 'ascension': 974,\n",
       " 'ascent': 975,\n",
       " 'ascribe': 976,\n",
       " 'ascribes': 977,\n",
       " 'ash': 978,\n",
       " 'ashamed': 979,\n",
       " 'asher': 980,\n",
       " 'ashes': 981,\n",
       " 'ashford': 982,\n",
       " 'ashore': 983,\n",
       " 'ashy': 984,\n",
       " 'asia': 985,\n",
       " 'aside': 986,\n",
       " 'ask': 987,\n",
       " 'askance': 988,\n",
       " 'asked': 989,\n",
       " 'asker': 990,\n",
       " 'asketh': 991,\n",
       " 'asking': 992,\n",
       " 'asks': 993,\n",
       " 'aslant': 994,\n",
       " 'asleep': 995,\n",
       " 'asmath': 996,\n",
       " 'asp': 997,\n",
       " 'aspect': 998,\n",
       " 'aspects': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a vocabulary as a dictionary, with the words as keys and the indices as values\n",
    "vocab = {word: i for i, word in enumerate(sorted(word_counts['counts'].keys()))}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8902236",
   "metadata": {},
   "source": [
    "### Word probability\n",
    "\n",
    "Given a word, calculate its probability using the following formula:\n",
    "\n",
    "$$P(w_i) = \\frac{C(w_i)}{M}$$\n",
    "where \n",
    "\n",
    "$C(w_i)$ is the total number of times $w_i$ appears in the corpus.\n",
    "\n",
    "$M$ is the total number of words in the corpus.\n",
    "\n",
    "For example, the probability of the word 'am' in the sentence **'I am happy because I am learning'** is:\n",
    "\n",
    "$$P(am) = \\frac{C(w_i)}{M} = \\frac {2}{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1aae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(word_counts):\n",
    "    \"\"\"\n",
    "    Calculates the word probabilities for a dictionary of word counts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a word_probs dictionary\n",
    "    word_probs = {}\n",
    "    \n",
    "    # Get the total number of words\n",
    "    total_words = word_counts['total']\n",
    "    \n",
    "    # Calculate the probability of each word\n",
    "    for word, count in word_counts['counts'].items():\n",
    "        word_probs[word] = count / total_words\n",
    "    \n",
    "    return word_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0d3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common words: [('the', 0.03317464333277255), ('and', 0.03142935298400985), ('i', 0.026265399715154233), ('to', 0.023780383655657683), ('of', 0.019826190142898055), ('a', 0.017266687813917887), ('you', 0.016322766985116872), ('my', 0.014721312191219096), ('that', 0.013642545529732222), ('in', 0.013040234143735384)]\n"
     ]
    }
   ],
   "source": [
    "# Get the word probabilities\n",
    "word_probs = word_probabilities(word_counts)\n",
    "\n",
    "# Print the top 10 most common words and their probabilities\n",
    "print(f'Top 10 most common words: {Counter(word_probs).most_common(10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f380f69",
   "metadata": {},
   "source": [
    "### String manipulation\n",
    "\n",
    "In this part, we would like to implement 4 functions:\n",
    "* `delete_letter`: given a word, it returns all the possible strings that have **one character removed**. \n",
    "* `switch_letter`: given a word, it returns all the possible strings that have **two adjacent letters switched**.\n",
    "* `replace_letter`: given a word, it returns all the possible strings that have **one character replaced by another different letter**.\n",
    "* `insert_letter`: given a word, it returns all the possible strings that have an **additional character inserted**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ea7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_letter(word):\n",
    "    \"\"\"\n",
    "    Returns all possible strings obtained by deleting a letter from the word.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Only delete if word has at least 1 character\n",
    "    if len(word) > 1:\n",
    "        # Initialize a list of words\n",
    "        delete_l = []\n",
    "        \n",
    "        # Iterate through each character in the word\n",
    "        for i in range(len(word)):\n",
    "            # Delete the character at index i\n",
    "            delete_l.append(word[:i] + word[i+1:])\n",
    "            \n",
    "        return list(set(delete_l))\n",
    "    else:\n",
    "        return [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a349f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ans', 'cas', 'can', 'cns']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "delete_letter('cans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09afe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word):\n",
    "    \"\"\"\n",
    "    Returns all possible strings obtained by switching two consecutive characters in the word.\n",
    "    Note: if the two characters are the same, we should not switch them.\n",
    "    \"\"\"\n",
    "    # Only switch if word has at least 2 characters\n",
    "    if len(word) > 1:\n",
    "        # Define a list of all possible words\n",
    "        switch_l = []\n",
    "        \n",
    "        # Loop through each character in the word\n",
    "        for i in range(len(word) - 1):\n",
    "            if word[i] != word[i+1]:\n",
    "                switch_l.append(word[:i] + word[i+1] + word[i] + word[i+2:])\n",
    "                \n",
    "        return list(set(switch_l))\n",
    "    else:\n",
    "        return [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c210b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tea', 'eat']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "switch_letter('eta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word):\n",
    "    \"\"\"\n",
    "    Returns all possible strings obtained by replacing one character in the word with another character.\n",
    "    \"\"\"\n",
    "    # Only replace if word has at least 1 character\n",
    "    if len(word) > 1:\n",
    "        # Define a list of all possible words\n",
    "        letters = string.ascii_lowercase\n",
    "        replace_l = []\n",
    "        \n",
    "        # Loop through each character in the word\n",
    "        for i in range(len(word)):\n",
    "            for letter in letters:\n",
    "                replace_l.append(word[:i] + letter + word[i+1:])\n",
    "                \n",
    "        return list(set(replace_l))\n",
    "    else:\n",
    "        return [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a201db11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cdn',\n",
       " 'cad',\n",
       " 'cae',\n",
       " 'gan',\n",
       " 'tan',\n",
       " 'cau',\n",
       " 'caq',\n",
       " 'kan',\n",
       " 'van',\n",
       " 'cav',\n",
       " 'cfn',\n",
       " 'caa',\n",
       " 'caf',\n",
       " 'ean',\n",
       " 'cap',\n",
       " 'crn',\n",
       " 'caj',\n",
       " 'ccn',\n",
       " 'dan',\n",
       " 'ckn',\n",
       " 'cmn',\n",
       " 'con',\n",
       " 'ctn',\n",
       " 'ran',\n",
       " 'man',\n",
       " 'cxn',\n",
       " 'cin',\n",
       " 'cai',\n",
       " 'cln',\n",
       " 'cam',\n",
       " 'yan',\n",
       " 'ian',\n",
       " 'cqn',\n",
       " 'caw',\n",
       " 'chn',\n",
       " 'cay',\n",
       " 'wan',\n",
       " 'cao',\n",
       " 'czn',\n",
       " 'cvn',\n",
       " 'cal',\n",
       " 'xan',\n",
       " 'zan',\n",
       " 'caz',\n",
       " 'lan',\n",
       " 'car',\n",
       " 'cun',\n",
       " 'cbn',\n",
       " 'san',\n",
       " 'cax',\n",
       " 'cag',\n",
       " 'oan',\n",
       " 'jan',\n",
       " 'nan',\n",
       " 'cnn',\n",
       " 'han',\n",
       " 'cab',\n",
       " 'cas',\n",
       " 'cat',\n",
       " 'qan',\n",
       " 'ban',\n",
       " 'cgn',\n",
       " 'cac',\n",
       " 'cah',\n",
       " 'cwn',\n",
       " 'csn',\n",
       " 'cjn',\n",
       " 'fan',\n",
       " 'cak',\n",
       " 'cpn',\n",
       " 'can',\n",
       " 'cen',\n",
       " 'uan',\n",
       " 'cyn',\n",
       " 'pan',\n",
       " 'aan']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "replace_letter('can')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365eae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word):\n",
    "    \"\"\"\n",
    "    Returns all possible strings obtained by inserting a new character in the word.\n",
    "    \"\"\"    \n",
    "    # Define a list of all possible words\n",
    "    letters = string.ascii_lowercase\n",
    "    insert_l = []\n",
    "    \n",
    "    # Loop through each character in the word\n",
    "    for i in range(len(word) + 1):\n",
    "        for letter in letters:\n",
    "            insert_l.append(word[:i] + letter + word[i:])\n",
    "        \n",
    "    return list(set(insert_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708a455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yat',\n",
       " 'akt',\n",
       " 'aat',\n",
       " 'vat',\n",
       " 'atg',\n",
       " 'aut',\n",
       " 'nat',\n",
       " 'adt',\n",
       " 'apt',\n",
       " 'ath',\n",
       " 'atu',\n",
       " 'aty',\n",
       " 'zat',\n",
       " 'atk',\n",
       " 'dat',\n",
       " 'art',\n",
       " 'aft',\n",
       " 'lat',\n",
       " 'amt',\n",
       " 'eat',\n",
       " 'fat',\n",
       " 'avt',\n",
       " 'aqt',\n",
       " 'atm',\n",
       " 'iat',\n",
       " 'act',\n",
       " 'ast',\n",
       " 'azt',\n",
       " 'alt',\n",
       " 'atf',\n",
       " 'atz',\n",
       " 'ats',\n",
       " 'atj',\n",
       " 'hat',\n",
       " 'wat',\n",
       " 'kat',\n",
       " 'xat',\n",
       " 'ant',\n",
       " 'aot',\n",
       " 'bat',\n",
       " 'qat',\n",
       " 'rat',\n",
       " 'atn',\n",
       " 'atx',\n",
       " 'ati',\n",
       " 'awt',\n",
       " 'atl',\n",
       " 'att',\n",
       " 'atw',\n",
       " 'cat',\n",
       " 'ate',\n",
       " 'atc',\n",
       " 'ajt',\n",
       " 'pat',\n",
       " 'abt',\n",
       " 'agt',\n",
       " 'ait',\n",
       " 'mat',\n",
       " 'uat',\n",
       " 'atd',\n",
       " 'atp',\n",
       " 'atv',\n",
       " 'tat',\n",
       " 'sat',\n",
       " 'aet',\n",
       " 'ayt',\n",
       " 'ato',\n",
       " 'oat',\n",
       " 'aht',\n",
       " 'atq',\n",
       " 'gat',\n",
       " 'atb',\n",
       " 'axt',\n",
       " 'atr',\n",
       " 'ata',\n",
       " 'jat']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "insert_letter('at')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5f2d8",
   "metadata": {},
   "source": [
    "### Edit n letters\n",
    "\n",
    "Now, we have implemented functions to edit the strings. Let's use them to edit one or two letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fc095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = False):\n",
    "    \"\"\"\n",
    "    Returns a set of all possible strings obtained by applying one edit to the word.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_l = []\n",
    "    \n",
    "    # All all possible words obtained by deleting a letter\n",
    "    edit_l.extend(delete_letter(word))\n",
    "    \n",
    "    # All all possible words obtained by replacing a letter\n",
    "    edit_l.extend(replace_letter(word))\n",
    "    \n",
    "    # All all possible words obtained by inserting a letter\n",
    "    edit_l.extend(insert_letter(word))\n",
    "    \n",
    "    # All all possible words obtained by switching two consecutive letters\n",
    "    if allow_switches:\n",
    "        edit_l.extend(switch_letter(word))\n",
    "        \n",
    "    return set(edit_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18603d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'aa',\n",
       " 'aat',\n",
       " 'ab',\n",
       " 'abt',\n",
       " 'ac',\n",
       " 'act',\n",
       " 'ad',\n",
       " 'adt',\n",
       " 'ae',\n",
       " 'aet',\n",
       " 'af',\n",
       " 'aft',\n",
       " 'ag',\n",
       " 'agt',\n",
       " 'ah',\n",
       " 'aht',\n",
       " 'ai',\n",
       " 'ait',\n",
       " 'aj',\n",
       " 'ajt',\n",
       " 'ak',\n",
       " 'akt',\n",
       " 'al',\n",
       " 'alt',\n",
       " 'am',\n",
       " 'amt',\n",
       " 'an',\n",
       " 'ant',\n",
       " 'ao',\n",
       " 'aot',\n",
       " 'ap',\n",
       " 'apt',\n",
       " 'aq',\n",
       " 'aqt',\n",
       " 'ar',\n",
       " 'art',\n",
       " 'as',\n",
       " 'ast',\n",
       " 'at',\n",
       " 'ata',\n",
       " 'atb',\n",
       " 'atc',\n",
       " 'atd',\n",
       " 'ate',\n",
       " 'atf',\n",
       " 'atg',\n",
       " 'ath',\n",
       " 'ati',\n",
       " 'atj',\n",
       " 'atk',\n",
       " 'atl',\n",
       " 'atm',\n",
       " 'atn',\n",
       " 'ato',\n",
       " 'atp',\n",
       " 'atq',\n",
       " 'atr',\n",
       " 'ats',\n",
       " 'att',\n",
       " 'atu',\n",
       " 'atv',\n",
       " 'atw',\n",
       " 'atx',\n",
       " 'aty',\n",
       " 'atz',\n",
       " 'au',\n",
       " 'aut',\n",
       " 'av',\n",
       " 'avt',\n",
       " 'aw',\n",
       " 'awt',\n",
       " 'ax',\n",
       " 'axt',\n",
       " 'ay',\n",
       " 'ayt',\n",
       " 'az',\n",
       " 'azt',\n",
       " 'bat',\n",
       " 'bt',\n",
       " 'cat',\n",
       " 'ct',\n",
       " 'dat',\n",
       " 'dt',\n",
       " 'eat',\n",
       " 'et',\n",
       " 'fat',\n",
       " 'ft',\n",
       " 'gat',\n",
       " 'gt',\n",
       " 'hat',\n",
       " 'ht',\n",
       " 'iat',\n",
       " 'it',\n",
       " 'jat',\n",
       " 'jt',\n",
       " 'kat',\n",
       " 'kt',\n",
       " 'lat',\n",
       " 'lt',\n",
       " 'mat',\n",
       " 'mt',\n",
       " 'nat',\n",
       " 'nt',\n",
       " 'oat',\n",
       " 'ot',\n",
       " 'pat',\n",
       " 'pt',\n",
       " 'qat',\n",
       " 'qt',\n",
       " 'rat',\n",
       " 'rt',\n",
       " 'sat',\n",
       " 'st',\n",
       " 't',\n",
       " 'tat',\n",
       " 'tt',\n",
       " 'uat',\n",
       " 'ut',\n",
       " 'vat',\n",
       " 'vt',\n",
       " 'wat',\n",
       " 'wt',\n",
       " 'xat',\n",
       " 'xt',\n",
       " 'yat',\n",
       " 'yt',\n",
       " 'zat',\n",
       " 'zt'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "edit_one_letter('at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = False):\n",
    "    \"\"\"\n",
    "    Returns all possible strings obtained by editing the word twice.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all possible words obtained by editing the word once\n",
    "    edit_l = edit_one_letter(word, allow_switches)\n",
    "    \n",
    "    # Define a set of all possible words obtained by editing the word twice\n",
    "    edit_2_set = set()\n",
    "    \n",
    "    # Add all possible words obtained by editing the word once, to the set of words obtained by editing the word twice\n",
    "    for edit_word in edit_l:\n",
    "        edit_2_set.update(edit_one_letter(edit_word, allow_switches))\n",
    "        \n",
    "    return edit_2_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings obtained by editing the word twice: 7129\n"
     ]
    }
   ],
   "source": [
    "edit_2 = edit_two_letters('at')\n",
    "print(f'Number of strings obtained by editing the word twice: {len(edit_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45421585",
   "metadata": {},
   "source": [
    "### Suggest spelling correction\n",
    "\n",
    "Given the 2 distance edit candidates, find the most probable word for suggestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, vocab, word_probs, k=2):\n",
    "    \"\"\"\n",
    "    Returns the k most probable words for spelling correction.\n",
    "    \n",
    "    Args:\n",
    "        word: The word to be corrected.\n",
    "        vocab: A dictionary containing all the words in the vocabulary.\n",
    "        word_probs: A dictionary containing the word probabilities.\n",
    "        k: The number of most probable words to return.\n",
    "        \n",
    "    Returns: a list of tuples of the form (word, probability).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the set of all possible words obtained by editing the word once\n",
    "    edit_one = edit_one_letter(word).intersection(vocab.keys())\n",
    "    \n",
    "    # Get the set of all possible words obtained by editing the word twice\n",
    "    edit_two = edit_two_letters(word).intersection(vocab.keys())\n",
    "    \n",
    "    # Define a set of all possible words\n",
    "    suggestions = edit_one.union(edit_two)\n",
    "    \n",
    "    # Also include the word itself if it is in the dictionary\n",
    "    if word in vocab:\n",
    "        suggestions.add(word)\n",
    "        \n",
    "    # Suggestions with their probabilities\n",
    "    suggestions_prob = {word: word_probs[word] for word in suggestions}\n",
    "    \n",
    "    # Get the k most probable suggestions\n",
    "    best_words = Counter(suggestions_prob).most_common(k)\n",
    "    \n",
    "    return best_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3495c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "get_corrections('dys', vocab, word_probs, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aace5ec",
   "metadata": {},
   "source": [
    "### Minimum edit distance\n",
    "\n",
    "Now we have implemented the simple autocorrect functions. How do we know how many steps it takes to modify a word from 'waht' to 'what'?\n",
    "\n",
    "In this part, we are implementing the minimum edit distance algorithm by applying dynamic programming. This breaks a problem down into subproblems which can be combined to form the final solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e732b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost=1, del_cost=1, rep_cost=2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m + 1, n + 1), dtype=int)\n",
    "\n",
    "    # Fill in column 0, from row 1 to row m, both inclusive\n",
    "    for row in range(1, m + 1):  # Replace None with the proper range\n",
    "        D[row, 0] = D[row - 1, 0] + del_cost\n",
    "\n",
    "    # Fill in row 0, for all columns from 1 to n, both inclusive\n",
    "    for col in range(1, n + 1):  # Replace None with the proper range\n",
    "        D[0, col] = D[0, col - 1] + ins_cost\n",
    "\n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1, m + 1):\n",
    "\n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1, n + 1):\n",
    "\n",
    "            # Intialize r_cost to the 'replace' cost that is passed into this function\n",
    "            r_cost = rep_cost\n",
    "\n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row - 1] == target[col - 1]:  # Replace None with a proper comparison\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "\n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row, col] = min([D[row - 1, col] + del_cost, D[row, col - 1] + ins_cost, D[row - 1, col - 1] + r_cost])\n",
    "\n",
    "    # Set the minimum edit distance with the cost found at row m, column n \n",
    "    med = D[m, n]\n",
    "\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ebe0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \", min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns=cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938553b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
